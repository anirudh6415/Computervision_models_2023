{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniru\\.conda\\envs\\maskdino\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# InternImage\n",
    "# Copyright (c) 2022 OpenGVLab\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "\n",
    "# from detectron2.utils.logger import setup_logger\n",
    "# from detectron2.modeling.backbone import Backbone\n",
    "\n",
    "import dcn_v3 as opsm\n",
    "\n",
    "\n",
    "class to_channels_first(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class to_channels_last(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "def build_norm_layer(dim,\n",
    "                     norm_layer,\n",
    "                     in_format='channels_last',\n",
    "                     out_format='channels_last',\n",
    "                     eps=1e-6):\n",
    "    layers = []\n",
    "    if norm_layer == 'BN':\n",
    "        if in_format == 'channels_last':\n",
    "            layers.append(to_channels_first())\n",
    "        layers.append(nn.BatchNorm2d(dim))\n",
    "        if out_format == 'channels_last':\n",
    "            layers.append(to_channels_last())\n",
    "    elif norm_layer == 'LN':\n",
    "        if in_format == 'channels_first':\n",
    "            layers.append(to_channels_last())\n",
    "        layers.append(nn.LayerNorm(dim, eps=eps))\n",
    "        if out_format == 'channels_first':\n",
    "            layers.append(to_channels_first())\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f'build_norm_layer does not support {norm_layer}')\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def build_act_layer(act_layer):\n",
    "    if act_layer == 'ReLU':\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif act_layer == 'SiLU':\n",
    "        return nn.SiLU(inplace=True)\n",
    "    elif act_layer == 'GELU':\n",
    "        return nn.GELU()\n",
    "\n",
    "    raise NotImplementedError(f'build_act_layer does not support {act_layer}')\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    r\"\"\" Cross Attention Module\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads. Default: 8\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to q, k, v.\n",
    "            Default: False.\n",
    "        qk_scale (float | None, optional): Override default qk scale of\n",
    "            head_dim ** -0.5 if set. Default: None.\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight.\n",
    "            Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "        attn_head_dim (int, optional): Dimension of attention head.\n",
    "        out_dim (int, optional): Dimension of output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 attn_head_dim=None,\n",
    "                 out_dim=None):\n",
    "        super().__init__()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        assert all_head_dim == dim\n",
    "\n",
    "        self.q = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.k = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, all_head_dim, bias=False)\n",
    "\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.k_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.k_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, out_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, k=None, v=None):\n",
    "        B, N, C = x.shape\n",
    "        N_k = k.shape[1]\n",
    "        N_v = v.shape[1]\n",
    "\n",
    "        q_bias, k_bias, v_bias = None, None, None\n",
    "        if self.q_bias is not None:\n",
    "            q_bias = self.q_bias\n",
    "            k_bias = self.k_bias\n",
    "            v_bias = self.v_bias\n",
    "\n",
    "        q = F.linear(input=x, weight=self.q.weight, bias=q_bias)\n",
    "        q = q.reshape(B, N, 1, self.num_heads,\n",
    "                      -1).permute(2, 0, 3, 1,\n",
    "                                  4).squeeze(0)  # (B, N_head, N_q, dim)\n",
    "\n",
    "        k = F.linear(input=k, weight=self.k.weight, bias=k_bias)\n",
    "        k = k.reshape(B, N_k, 1, self.num_heads, -1).permute(2, 0, 3, 1,\n",
    "                                                             4).squeeze(0)\n",
    "\n",
    "        v = F.linear(input=v, weight=self.v.weight, bias=v_bias)\n",
    "        v = v.reshape(B, N_v, 1, self.num_heads, -1).permute(2, 0, 3, 1,\n",
    "                                                             4).squeeze(0)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # (B, N_head, N_q, N_k)\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentiveBlock(nn.Module):\n",
    "    r\"\"\"Attentive Block\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads. Default: 8\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to q, k, v.\n",
    "            Default: False.\n",
    "        qk_scale (float | None, optional): Override default qk scale of\n",
    "            head_dim ** -0.5 if set. Default: None.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0.\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0.\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate.\n",
    "            Default: 0.0.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm.\n",
    "        attn_head_dim (int, optional): Dimension of attention head. Default: None.\n",
    "        out_dim (int, optional): Dimension of output. Default: None.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=\"LN\",\n",
    "                 attn_head_dim=None,\n",
    "                 out_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1_q = build_norm_layer(dim, norm_layer, eps=1e-6)\n",
    "        self.norm1_k = build_norm_layer(dim, norm_layer, eps=1e-6)\n",
    "        self.norm1_v = build_norm_layer(dim, norm_layer, eps=1e-6)\n",
    "        self.cross_dcn = CrossAttention(dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        qkv_bias=qkv_bias,\n",
    "                                        qk_scale=qk_scale,\n",
    "                                        attn_drop=attn_drop,\n",
    "                                        proj_drop=drop,\n",
    "                                        attn_head_dim=attn_head_dim,\n",
    "                                        out_dim=out_dim)\n",
    "\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self,\n",
    "                x_q,\n",
    "                x_kv,\n",
    "                pos_q,\n",
    "                pos_k,\n",
    "                bool_masked_pos,\n",
    "                rel_pos_bias=None):\n",
    "        x_q = self.norm1_q(x_q + pos_q)\n",
    "        x_k = self.norm1_k(x_kv + pos_k)\n",
    "        x_v = self.norm1_v(x_kv)\n",
    "\n",
    "        x = self.cross_dcn(x_q, k=x_k, v=x_v)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionPoolingBlock(AttentiveBlock):\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_q = x.mean(1, keepdim=True)\n",
    "        x_kv = x\n",
    "        pos_q, pos_k = 0, 0\n",
    "        x = super().forward(x_q, x_kv, pos_q, pos_k,\n",
    "                            bool_masked_pos=None,\n",
    "                            rel_pos_bias=None)\n",
    "        x = x.squeeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StemLayer(nn.Module):\n",
    "    r\"\"\" Stem layer of InternImage\n",
    "    Args:\n",
    "        in_chans (int): number of input channels\n",
    "        out_chans (int): number of output channels\n",
    "        act_layer (str): activation layer\n",
    "        norm_layer (str): normalization layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 out_chans=96,\n",
    "                 act_layer='GELU',\n",
    "                 norm_layer='BN'):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans,\n",
    "                               out_chans // 2,\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "        self.norm1 = build_norm_layer(out_chans // 2, norm_layer,\n",
    "                                      'channels_first', 'channels_first')\n",
    "        self.act = build_act_layer(act_layer)\n",
    "        self.conv2 = nn.Conv2d(out_chans // 2,\n",
    "                               out_chans,\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "        self.norm2 = build_norm_layer(out_chans, norm_layer, 'channels_first',\n",
    "                                      'channels_last')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownsampleLayer(nn.Module):\n",
    "    r\"\"\" Downsample layer of InternImage\n",
    "    Args:\n",
    "        channels (int): number of input channels\n",
    "        norm_layer (str): normalization layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, norm_layer='LN'):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels,\n",
    "                              2 * channels,\n",
    "                              kernel_size=3,\n",
    "                              stride=2,\n",
    "                              padding=1,\n",
    "                              bias=False)\n",
    "        self.norm = build_norm_layer(2 * channels, norm_layer,\n",
    "                                     'channels_first', 'channels_last')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x.permute(0, 3, 1, 2))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    r\"\"\" MLP layer of InternImage\n",
    "    Args:\n",
    "        in_features (int): number of input features\n",
    "        hidden_features (int): number of hidden features\n",
    "        out_features (int): number of output features\n",
    "        act_layer (str): activation layer\n",
    "        drop (float): dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer='GELU',\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = build_act_layer(act_layer)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InternImageLayer(nn.Module):\n",
    "    r\"\"\" Basic layer of InternImage\n",
    "    Args:\n",
    "        core_op (nn.Module): core operation of InternImage\n",
    "        channels (int): number of input channels\n",
    "        groups (list): Groups of each block.\n",
    "        mlp_ratio (float): ratio of mlp hidden features to input channels\n",
    "        drop (float): dropout rate\n",
    "        drop_path (float): drop path rate\n",
    "        act_layer (str): activation layer\n",
    "        norm_layer (str): normalization layer\n",
    "        post_norm (bool): whether to use post normalization\n",
    "        layer_scale (float): layer scale\n",
    "        offset_scale (float): offset scale\n",
    "        with_cp (bool): whether to use checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 core_op,\n",
    "                 channels,\n",
    "                 groups,\n",
    "                 mlp_ratio=4.,\n",
    "                 drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer='GELU',\n",
    "                 norm_layer='LN',\n",
    "                 post_norm=False,\n",
    "                 layer_scale=None,\n",
    "                 offset_scale=1.0,\n",
    "                 with_cp=False,\n",
    "                 dw_kernel_size=None, # for InternImage-H/G\n",
    "                 res_post_norm=False, # for InternImage-H/G\n",
    "                 center_feature_scale=False): # for InternImage-H/G\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.groups = groups\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        self.norm1 = build_norm_layer(channels, 'LN')\n",
    "        self.post_norm = post_norm\n",
    "        self.dcn = core_op(\n",
    "            channels=channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            pad=1,\n",
    "            dilation=1,\n",
    "            group=groups,\n",
    "            offset_scale=offset_scale,\n",
    "            act_layer=act_layer,\n",
    "            norm_layer=norm_layer,\n",
    "            dw_kernel_size=dw_kernel_size, # for InternImage-H/G\n",
    "            center_feature_scale=center_feature_scale) # for InternImage-H/G\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. \\\n",
    "            else nn.Identity()\n",
    "        self.norm2 = build_norm_layer(channels, 'LN')\n",
    "        self.mlp = MLPLayer(in_features=channels,\n",
    "                            hidden_features=int(channels * mlp_ratio),\n",
    "                            act_layer=act_layer,\n",
    "                            drop=drop)\n",
    "        self.layer_scale = layer_scale is not None\n",
    "        if self.layer_scale:\n",
    "            self.gamma1 = nn.Parameter(layer_scale * torch.ones(channels),\n",
    "                                       requires_grad=True)\n",
    "            self.gamma2 = nn.Parameter(layer_scale * torch.ones(channels),\n",
    "                                       requires_grad=True)\n",
    "        self.res_post_norm = res_post_norm\n",
    "        if res_post_norm:\n",
    "            self.res_post_norm1 = build_norm_layer(channels, 'LN')\n",
    "            self.res_post_norm2 = build_norm_layer(channels, 'LN')\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        def _inner_forward(x):\n",
    "            if not self.layer_scale:\n",
    "                if self.post_norm:\n",
    "                    x = x + self.drop_path(self.norm1(self.dcn(x)))\n",
    "                    x = x + self.drop_path(self.norm2(self.mlp(x)))\n",
    "                elif self.res_post_norm: # for InternImage-H/G\n",
    "                    x = x + self.drop_path(self.res_post_norm1(self.dcn(self.norm1(x))))\n",
    "                    x = x + self.drop_path(self.res_post_norm2(self.mlp(self.norm2(x))))\n",
    "                else:\n",
    "                    x = x + self.drop_path(self.dcn(self.norm1(x)))\n",
    "                    x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "                return x\n",
    "            if self.post_norm:\n",
    "                x = x + self.drop_path(self.gamma1 * self.norm1(self.dcn(x)))\n",
    "                x = x + self.drop_path(self.gamma2 * self.norm2(self.mlp(x)))\n",
    "            else:\n",
    "                x = x + self.drop_path(self.gamma1 * self.dcn(self.norm1(x)))\n",
    "                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))\n",
    "            return x\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            x = checkpoint.checkpoint(_inner_forward, x)\n",
    "        else:\n",
    "            x = _inner_forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InternImageBlock(nn.Module):\n",
    "    r\"\"\" Block of InternImage\n",
    "    Args:\n",
    "        core_op (nn.Module): core operation of InternImage\n",
    "        channels (int): number of input channels\n",
    "        depths (list): Depth of each block.\n",
    "        groups (list): Groups of each block.\n",
    "        mlp_ratio (float): ratio of mlp hidden features to input channels\n",
    "        drop (float): dropout rate\n",
    "        drop_path (float): drop path rate\n",
    "        act_layer (str): activation layer\n",
    "        norm_layer (str): normalization layer\n",
    "        post_norm (bool): whether to use post normalization\n",
    "        layer_scale (float): layer scale\n",
    "        offset_scale (float): offset scale\n",
    "        with_cp (bool): whether to use checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 core_op,\n",
    "                 channels,\n",
    "                 depth,\n",
    "                 groups,\n",
    "                 downsample=True,\n",
    "                 mlp_ratio=4.,\n",
    "                 drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer='GELU',\n",
    "                 norm_layer='LN',\n",
    "                 post_norm=False,\n",
    "                 offset_scale=1.0,\n",
    "                 layer_scale=None,\n",
    "                 with_cp=False,\n",
    "                 dw_kernel_size=None, # for InternImage-H/G\n",
    "                 post_norm_block_ids=None, # for InternImage-H/G\n",
    "                 res_post_norm=False, # for InternImage-H/G\n",
    "                 center_feature_scale=False): # for InternImage-H/G\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.depth = depth\n",
    "        self.post_norm = post_norm\n",
    "        self.center_feature_scale = center_feature_scale\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            InternImageLayer(\n",
    "                core_op=core_op,\n",
    "                channels=channels,\n",
    "                groups=groups,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                drop=drop,\n",
    "                drop_path=drop_path[i] if isinstance(\n",
    "                    drop_path, list) else drop_path,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                post_norm=post_norm,\n",
    "                layer_scale=layer_scale,\n",
    "                offset_scale=offset_scale,\n",
    "                with_cp=with_cp,\n",
    "                dw_kernel_size=dw_kernel_size, # for InternImage-H/G\n",
    "                res_post_norm=res_post_norm, # for InternImage-H/G\n",
    "                center_feature_scale=center_feature_scale # for InternImage-H/G\n",
    "            ) for i in range(depth)\n",
    "        ])\n",
    "        if not self.post_norm or center_feature_scale:\n",
    "            self.norm = build_norm_layer(channels, 'LN')\n",
    "        self.post_norm_block_ids = post_norm_block_ids\n",
    "        if post_norm_block_ids is not None: # for InternImage-H/G\n",
    "            self.post_norms = nn.ModuleList(\n",
    "                [build_norm_layer(channels, 'LN', eps=1e-6) for _ in post_norm_block_ids]\n",
    "            )\n",
    "        self.downsample = DownsampleLayer(\n",
    "            channels=channels, norm_layer=norm_layer) if downsample else None\n",
    "\n",
    "    def forward(self, x, return_wo_downsample=False):\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if (self.post_norm_block_ids is not None) and (i in self.post_norm_block_ids):\n",
    "                index = self.post_norm_block_ids.index(i)\n",
    "                x = self.post_norms[index](x) # for InternImage-H/G\n",
    "        if not self.post_norm or self.center_feature_scale:\n",
    "            x = self.norm(x)\n",
    "        if return_wo_downsample:\n",
    "            x_ = x\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        if return_wo_downsample:\n",
    "            return x, x_\n",
    "        return x\n",
    "\n",
    "class InternImage(nn.Module):\n",
    "    r\"\"\" InternImage\n",
    "        A PyTorch impl of : `InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "    Args:\n",
    "        core_op (str): Core operator. Default: 'DCNv3'\n",
    "        channels (int): Number of the first stage. Default: 64\n",
    "        depths (list): Depth of each block. Default: [3, 4, 18, 5]\n",
    "        groups (list): Groups of each block. Default: [3, 6, 12, 24]\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        drop_rate (float): Probability of an element to be zeroed. Default: 0.\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        act_layer (str): Activation layer. Default: 'GELU'\n",
    "        norm_layer (str): Normalization layer. Default: 'LN'\n",
    "        layer_scale (bool): Whether to use layer scale. Default: False\n",
    "        cls_scale (bool): Whether to use class scale. Default: False\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "        dw_kernel_size (int): Size of the dwconv. Default: None\n",
    "        level2_post_norm (bool): Whether to use level2 post norm. Default: False\n",
    "        level2_post_norm_block_ids (list): Indexes of post norm blocks. Default: None\n",
    "        res_post_norm (bool): Whether to use res post norm. Default: False\n",
    "        center_feature_scale (bool): Whether to use center feature scale. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 core_op='DCNv3',\n",
    "                 channels=64,\n",
    "                 depths=[3, 4, 18, 5],\n",
    "                 groups=[3, 6, 12, 24],\n",
    "                 num_classes=1000,\n",
    "                 mlp_ratio=4.,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.2,\n",
    "                 drop_path_type='linear',\n",
    "                 act_layer='GELU',\n",
    "                 norm_layer='LN',\n",
    "                 layer_scale=None,\n",
    "                 offset_scale=1.0,\n",
    "                 post_norm=False,\n",
    "                 cls_scale=1.5,\n",
    "                 with_cp=False,\n",
    "                 dw_kernel_size=None,  # for InternImage-H/G\n",
    "                 level2_post_norm=False,  # for InternImage-H/G\n",
    "                 level2_post_norm_block_ids=None,  # for InternImage-H/G\n",
    "                 res_post_norm=False,  # for InternImage-H/G\n",
    "                 center_feature_scale=False,  # for InternImage-H/G\n",
    "                 out_indices=(0, 1, 2, 3),\n",
    "                 init_cfg=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.core_op = core_op\n",
    "        self.num_classes = num_classes\n",
    "        self.num_levels = len(depths)\n",
    "        self.depths = depths\n",
    "        self.channels = channels\n",
    "        self.num_features = int(channels * 2**(self.num_levels - 1))\n",
    "        self.post_norm = post_norm\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.init_cfg = init_cfg\n",
    "        self.out_indices = out_indices\n",
    "        self.level2_post_norm_block_ids = level2_post_norm_block_ids\n",
    "        print(\"InternImage\")\n",
    "        print(f'using core type: {core_op}')\n",
    "        print(f'using activation layer: {act_layer}')\n",
    "        print(f'using main norm layer: {norm_layer}')\n",
    "        print(f'using dpr: {drop_path_type}, {drop_path_rate}')\n",
    "        print(f\"level2_post_norm: {level2_post_norm}\")\n",
    "        print(f\"level2_post_norm_block_ids: {level2_post_norm_block_ids}\")\n",
    "        print(f\"res_post_norm: {res_post_norm}\")\n",
    "\n",
    "        in_chans = 3\n",
    "        self.patch_embed = StemLayer(in_chans=in_chans,\n",
    "                                     out_chans=channels,\n",
    "                                     act_layer=act_layer,\n",
    "                                     norm_layer=norm_layer)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]\n",
    "        if drop_path_type == 'uniform':\n",
    "            for i in range(len(dpr)):\n",
    "                dpr[i] = drop_path_rate\n",
    "\n",
    "        self.levels = nn.ModuleList()\n",
    "        for i in range(self.num_levels):\n",
    "            post_norm_block_ids = level2_post_norm_block_ids if level2_post_norm and (\n",
    "                i == 2) else None # for InternImage-H/G\n",
    "            level = InternImageBlock(\n",
    "                core_op=getattr(opsm, core_op),\n",
    "                channels=int(channels * 2**i),\n",
    "                depth=depths[i],\n",
    "                groups=groups[i],\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                drop=drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                post_norm=post_norm,\n",
    "                downsample=(i < self.num_levels - 1),\n",
    "                layer_scale=layer_scale,\n",
    "                offset_scale=offset_scale,\n",
    "                with_cp=with_cp,\n",
    "                dw_kernel_size=dw_kernel_size,  # for InternImage-H/G\n",
    "                post_norm_block_ids=post_norm_block_ids, # for InternImage-H/G\n",
    "                res_post_norm=res_post_norm, # for InternImage-H/G\n",
    "                center_feature_scale=center_feature_scale # for InternImage-H/G\n",
    "            )\n",
    "            self.levels.append(level)\n",
    "\n",
    "        self.conv_head = nn.Sequential(\n",
    "                nn.Conv2d(self.num_features,\n",
    "                          int(self.num_features * cls_scale),\n",
    "                          kernel_size=1,\n",
    "                          bias=False),\n",
    "                build_norm_layer(int(self.num_features * cls_scale), 'BN',\n",
    "                                 'channels_first', 'channels_first'),\n",
    "                build_act_layer(act_layer))\n",
    "        self.head = nn.Linear(int(self.num_features * cls_scale), num_classes) \\\n",
    "                if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.num_layers = len(depths)\n",
    "        self.apply(self._init_weights)\n",
    "        self.apply(self._init_deform_weights)\n",
    "\n",
    "        # self.num_layers = len(depths)\n",
    "        # self.apply(self._init_weights)\n",
    "        # self.apply(self._init_deform_weights)\n",
    "\n",
    "        # # add basic info for d2 backbone\n",
    "        # self._out_features = [\"p{}\".format(i) for i in self.out_indices]\n",
    "        # self._out_feature_channels = {\n",
    "        #     \"p{}\".format(i): self.channels * 2**i for i in self.out_indices\n",
    "        # }\n",
    "        # self._out_feature_strides = {\"p{}\".format(i): 2 ** (i + 2) for i in self.out_indices}\n",
    "        # self._size_devisibility = 32\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def _init_deform_weights(self, m):\n",
    "        if isinstance(m, getattr(opsm, self.core_op)):\n",
    "            m._reset_parameters()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for level in self.levels:\n",
    "            x = level(x)\n",
    "\n",
    "        x = self.conv_head(x.permute(0, 3, 1, 2))\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward_features_seq_out(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        seq_out = []\n",
    "        for level in self.levels:\n",
    "            x, x_ = level(x, return_wo_downsample=True)\n",
    "            seq_out.append(x_)\n",
    "        return seq_out\n",
    "        # for level_idx, level in enumerate(self.levels):\n",
    "        #     x, x_ = level(x, return_wo_downsample=True)\n",
    "        #     if level_idx in self.out_indices:\n",
    "                # seq_out.append(x_.permute(0, 3, 1, 2).contiguous())\n",
    "                # seq_out[\"p{}\".format(level_idx)] = x_.permute(0, 3, 1, 2).contiguous()\n",
    "        # return seq_out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # if self.use_clip_projector: # for InternImage-H/G\n",
    "        #     x = self.forward_clip_projector(x)\n",
    "        # else: # for InternImage-T/S/B/L/XL\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InternImage\n",
      "using core type: DCNv3\n",
      "using activation layer: GELU\n",
      "using main norm layer: LN\n",
      "using dpr: linear, 0.1\n",
      "level2_post_norm: False\n",
      "level2_post_norm_block_ids: None\n",
      "res_post_norm: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://huggingface.co/OpenGVLab/InternImage/resolve/main/internimage_t_1k_224.pth\" to C:\\Users\\aniru/.cache\\torch\\hub\\checkpoints\\internimage_t_1k_224.pth\n",
      "100%|██████████| 114M/114M [00:10<00:00, 11.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InternImage_Tmodel loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def main(model_name):\n",
    "    # Create InternImage Tiny model\n",
    "    if model_name == 'InternImage_T':\n",
    "        model= InternImage(channels=64,\n",
    "                             depths=[4, 4, 18, 4],\n",
    "                             groups=[4, 8, 16, 32],\n",
    "                             offset_scale=1.0,\n",
    "                             drop_path_rate=0.1,\n",
    "                             post_norm=False)\n",
    "        weights_url = \"https://huggingface.co/OpenGVLab/InternImage/resolve/main/internimage_t_1k_224.pth\"\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(weights_url)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    elif model_name == 'InternImage_S':\n",
    "        model= InternImage(channels=64,\n",
    "                              depths=[4, 4, 18, 4],\n",
    "                              groups=[4, 8, 16, 32],\n",
    "                              offset_scale=1.0,\n",
    "                              drop_path_rate=0.1,\n",
    "                              post_norm=False)\n",
    "        weights_url = \"https://huggingface.co/OpenGVLab/InternImage/resolve/main/internimage_s_1k_224.pth\"\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(weights_url)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    elif model_name == 'InternImage_B':\n",
    "        model= InternImage(channels=112,\n",
    "                             depths=[4, 4, 21, 4],\n",
    "                             groups=[7, 14, 28, 56],\n",
    "                             offset_scale=1.0,\n",
    "                             drop_path_rate=0.1,\n",
    "                             post_norm=True)\n",
    "        weights_url = \"https://huggingface.co/OpenGVLab/InternImage/resolve/main/internimage_b_1k_224.pth\"\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(weights_url)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    elif model_name == 'InternImage_L':\n",
    "        model= InternImage(channels=160,\n",
    "                              depths=[5, 5, 22, 5],\n",
    "                              groups=[10, 20, 40, 80],\n",
    "                              offset_scale=2.0,\n",
    "                              drop_path_rate=0.0,\n",
    "                              post_norm=True)\n",
    "        weights_url = \"https://huggingface.co/OpenGVLab/InternImage/resolve/main/internimage_l_22kto1k_384.pth\"\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(weights_url)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"InternImage_T\"\n",
    "    model = main(model_name)\n",
    "    print(f\"{model_name}model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskdino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
